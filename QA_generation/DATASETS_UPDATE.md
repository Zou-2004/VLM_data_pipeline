# QA Generation - New Datasets Added

## Overview
Added support for **Hypersim** and **Taskonomy** datasets to the QA generation pipeline. Both datasets now support all 3D spatial reasoning tasks.

## New Datasets

### 1. Hypersim
**Source:** `processed_data/hypersim/`

**Characteristics:**
- Photorealistic synthetic indoor scenes
- High-quality depth maps (HDF5 format)
- Full camera intrinsics and extrinsics
- Semantic instance segmentation with generic labels (e.g., `floor_tile_obj_17`)

**Data Format:**
```json
{
  "dataset": "hypersim",
  "split": "ai_001_001",
  "image_id": "ai_001_001_cam_00_frame_0025",
  "filename": "frame.0025.color.hdf5",
  "camera": { /* Full intrinsics + extrinsics */ },
  "depth_stats": { /* Depth map statistics */ },
  "bounding_boxes_3d": [
    {
      "x": 132.59, "y": 9.54, "z": 7.52,
      "xl": 20.43, "yl": 13.28, "zl": 15.28,
      "category": "floor_tile_obj_17"
    }
  ]
}
```

**Supported Tasks:**
- ✅ object_count
- ✅ object_3d_size
- ✅ obj_obj_distance
- ✅ cam_obj_distance
- ✅ cam_obj_rel_dist
- ✅ obj_obj_rel_pos

### 2. Taskonomy (Labeled)
**Source:** `processed_data/taskonomy_labeled/`

**Important:** This dataset uses the **labeled** version created by Enhanced CLIP pipeline (v1 or v2), not the raw Taskonomy data.

**Characteristics:**
- Real-world indoor scenes from university buildings
- Depth maps (PNG encoded)
- Full camera intrinsics and extrinsics
- **Semantic labels from Enhanced CLIP** (e.g., `shelf`, `monitor`, `book`, `stool`)
- High-quality precise 2D bounding boxes extracted from instance masks

**Data Format:**
```json
{
  "dataset": "taskonomy",
  "split": "ackermanville",
  "image_id": "ackermanville_point_115_view_11",
  "filename": "point_115_view_11_domain_rgb.png",
  "camera": { /* Full intrinsics + extrinsics */ },
  "depth_stats": { /* Depth map statistics */ },
  "bounding_boxes_2d": [
    {
      "x_min": 41, "y_min": 0, "x_max": 55, "y_max": 54,
      "instance_id": 2,
      "category": "column",
      "object_id": "column_2"
    }
  ],
  "bounding_boxes_3d": [
    {
      "x": -1.10, "y": -1.21, "z": 2.80,
      "xl": 0.19, "yl": 0.10, "zl": 0.42,
      "category": "column",
      "object_id": "column_2"
    }
  ],
  "labeling_info": {
    "method": "enhanced_clip_pipeline_v1",
    "version": "v1",
    "labeling_success_rate": 0.93
  }
}
```

**Supported Tasks:**
- ✅ object_count
- ✅ object_3d_size
- ✅ obj_obj_distance
- ✅ cam_obj_distance
- ✅ cam_obj_rel_dist
- ✅ obj_obj_rel_pos

**Label Quality:**
- Labels generated by Enhanced CLIP v1 (threshold-based) or v2 (hierarchical with agreement)
- Success rate: ~92-96% of objects labeled
- Common labels: shelf, monitor, book, blackboard, tv, chair, stool, column, door, window

## Configuration Changes

### Updated `config.py`

Added two new dataset configurations:

```python
DATASETS = {
    # ... existing datasets ...
    
    "hypersim": {
        "data_dir": os.path.join(PROCESSED_DATA_DIR, "hypersim"),
        "has_3d": True,
        "has_camera_poses": True,
        "has_depth": True,
        "tasks": [
            "object_count",
            "object_3d_size",
            "obj_obj_distance",
            "cam_obj_distance",
            "cam_obj_rel_dist",
            "obj_obj_rel_pos",
        ]
    },
    "taskonomy": {
        "data_dir": os.path.join(PROCESSED_DATA_DIR, "taskonomy_labeled"),
        "has_3d": True,
        "has_camera_poses": True,
        "has_depth": True,
        "tasks": [
            "object_count",
            "object_3d_size",
            "obj_obj_distance",
            "cam_obj_distance",
            "cam_obj_rel_dist",
            "obj_obj_rel_pos",
        ]
    }
}
```

**Key Points:**
- **Taskonomy uses `taskonomy_labeled/` directory** (not `taskonomy/`)
- Both datasets have depth maps
- Both datasets support all 3D spatial reasoning tasks

## Usage Examples

### Generate QA for Hypersim
```bash
# Generate all tasks for Hypersim
python generate_qa.py --dataset hypersim

# Generate specific tasks
python generate_qa.py --dataset hypersim --tasks object_count object_3d_size

# Test with limited files
python generate_qa.py --dataset hypersim --limit 100
```

### Generate QA for Taskonomy (Labeled)
```bash
# Generate all tasks for Taskonomy
python generate_qa.py --dataset taskonomy

# Generate specific tasks
python generate_qa.py --dataset taskonomy --tasks cam_obj_distance obj_obj_rel_pos

# Test with limited files
python generate_qa.py --dataset taskonomy --limit 50
```

### Generate for All Datasets
```bash
# Generate QA for all 5 datasets
python generate_qa.py --all

# This will process:
# - objectron
# - matterport
# - sunrgbd
# - hypersim (NEW)
# - taskonomy (NEW)
```

## Output Structure

After running QA generation, the output directory will have:

```
output/
├── objectron/
│   ├── object_count.json
│   ├── object_3d_size.json
│   ├── cam_obj_distance.json
│   ├── obj_obj_distance.json
│   ├── cam_obj_rel_dist.json
│   ├── obj_obj_rel_pos.json
│   ├── all_qa_pairs.json
│   └── summary.json
├── matterport/
│   └── ...
├── sunrgbd/
│   └── ...
├── hypersim/          ← NEW
│   ├── object_count.json
│   ├── object_3d_size.json
│   ├── cam_obj_distance.json
│   ├── obj_obj_distance.json
│   ├── cam_obj_rel_dist.json
│   ├── obj_obj_rel_pos.json
│   ├── all_qa_pairs.json
│   └── summary.json
└── taskonomy/         ← NEW
    ├── object_count.json
    ├── object_3d_size.json
    ├── cam_obj_distance.json
    ├── obj_obj_distance.json
    ├── cam_obj_rel_dist.json
    ├── obj_obj_rel_pos.json
    ├── all_qa_pairs.json
    └── summary.json
```

## Data Availability

### Check Dataset Sizes
```bash
# Count Hypersim JSON files
find processed_data/hypersim -name "*.json" -type f | wc -l

# Count Taskonomy labeled JSON files (exclude summary)
find processed_data/taskonomy_labeled -name "*.json" -type f | grep -v summary | wc -l
```

### Prerequisites

**For Taskonomy:**
1. Must have run Enhanced CLIP pipeline first:
   ```bash
   cd data_processing
   
   # Option 1: Use v1 (threshold-based)
   python build_enhanced_codebook.py
   
   # Option 2: Use v2 (hierarchical with agreement) - RECOMMENDED
   python build_enhanced_codebook_v2.py
   ```

2. Must have created labeled dataset:
   ```bash
   python create_labeled_dataset.py
   ```

3. This creates `processed_data/taskonomy_labeled/` with semantic labels

**For Hypersim:**
- Must have run Hypersim processor:
  ```bash
  cd data_processing
  python hypersim_processor.py
  ```

## Expected Output Statistics

### Hypersim
- **Scenes:** Multiple synthetic indoor scenes
- **Frames:** Varies by scene (typically 25-100 frames per camera)
- **Objects per frame:** 5-20 (generic labels like `floor_tile_obj_N`)
- **QA pairs:** Estimated 10K-50K depending on data size

### Taskonomy
- **Locations:** e.g., ackermanville
- **Views:** ~3,857 labeled views (from original 3,862)
- **Objects per view:** Average 20-30 (shelf, monitor, book most common)
- **Labeled instances:** 253 unique objects (96.4% labeled by Enhanced CLIP)
- **Total labeled bboxes:** ~86,550
- **QA pairs:** Estimated 50K-100K

## Label Distribution (Taskonomy)

Top semantic labels from Enhanced CLIP:
- `shelf` - 71 instances
- `blackboard` - 44 instances
- `monitor` - 21 instances
- `book` - 18 instances
- `tv` - 14 instances
- `column` - 12 instances
- `stool` - 11 instances
- `door` - 9 instances
- Others: cabinet, refrigerator, window, chair, etc.

## Quality Notes

### Hypersim
✅ **Strengths:**
- Perfect depth maps (synthetic data)
- Accurate camera poses
- Consistent lighting

⚠️ **Limitations:**
- Generic object labels (`floor_tile_obj_N`)
- May need label enhancement in future

### Taskonomy
✅ **Strengths:**
- Real-world data
- Semantic labels from Enhanced CLIP
- High labeling success rate (96.4%)
- Precise 2D bboxes from instance masks

⚠️ **Limitations:**
- ~3.6% of objects not labeled (rejected by CLIP pipeline)
- Depth maps encoded in PNG (less precise than raw)

## Troubleshooting

### Issue: "No data found in processed_data/taskonomy_labeled"
**Solution:** Run the Enhanced CLIP pipeline first:
```bash
cd data_processing
python build_enhanced_codebook_v2.py  # Generate labels
python create_labeled_dataset.py      # Create labeled dataset
```

### Issue: "No enhanced codebook found"
**Solution:** You need to run either v1 or v2 codebook builder first:
```bash
cd data_processing
python build_enhanced_codebook_v2.py  # Recommended (better quality)
# OR
python build_enhanced_codebook.py     # v1 (faster but lower precision)
```

### Issue: Hypersim depth format errors
**Solution:** Hypersim uses HDF5 format for depth. Ensure h5py is installed:
```bash
pip install h5py
```

## Summary

**Added Datasets:** 2 new datasets (Hypersim, Taskonomy)  
**New Tasks Supported:** All 6 3D tasks for both datasets  
**Total Datasets:** 5 (Objectron, Matterport, SUN RGB-D, Hypersim, Taskonomy)  
**Expected New QA Pairs:** 60K-150K combined from both datasets  

Both datasets are now fully integrated into the QA generation pipeline and ready for use!
